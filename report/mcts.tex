\subsection {Introduction}

Monte-Carlo tree search (MCTS) has become an umbrella term for a class of related algorithms for searching trees probabilistically.
This applies directly to games if we decide to search the game tree.

In this section, we will introduce an MCTS algorithm known as UCT.
We will mostly follow the exposition in \citep{mcts_survey12}, chapter 3.

\subsection {MCTS in general}

MCTS studies the game tree as follows.
It keeps a record of a subtree of the game-tree containing the nodes that the algorithm has visited so far.
It also keeps some extra information about each node, which is supposed to represent an approximation of the ``value'' of that node.
The idea is to somehow find a good \emph{expandable node} (meaning that it has unvisited children) in the visited part of the game tree, and then to make an excursion from that node, which means doing a quicker kind of search from the node, in order to estimate the value of the node. The information gleaned from this excursion will then contribute to the algorithms knowledge of the game tree.

It is assumed that we have a (reasonably efficient) function that lets us determine the value of a leaf node.
Here is a sketch of the steps that will make up our algorithm:

\begin{itemize}
\item \emph{Selection}: find a suitable expandable explored node, and select one of it's child nodes.
\item \emph{Exploration}: run a simulation from the newly found child node and return a \emph{score}.
\item \emph{Backpropagation}: use the score found in the previous step to update the visited tree in an appropriate way.
\end{itemize}
These steps are iterated a number of times in order to make a single move. Each iteration yields a more complete and refined knowledge of the game tree, thanks to the backpropagation step. In order to subsequently make a move, a sort of ``single level'' version of the selection step is carried out.
Note that there are variants of this algorithm which expand and explore multiple nodes instead of just one, but the principle is the same otherwise.
Note also that this algorithm is far from complete. There are various appropriate ways of performing each of these steps, depending on the situation.
The next section describes one of the possibilities: the UCT (Upper Confidence bounds for Trees) algorithm.

\subsection{The UCT algorithm}
\label{sec:uct}

In this section, we fill in each of the steps outlined in the previous section, for the special case of the UCT algorithm.

Each node $v$ in the explored part of the game tree has an attached score, which is just a real-valued number, say $S(v)$.


\subsubsection{The selection step}
\label{subsec:uct_selection_step}

Selection takes place in the explored part of the game tree, and can therefore use the score, $S$.
We repeatedly pick the ``best child'' of the current node, in the following sense.
If $v$ has children which have not been explored, then pick any of them as the best child.
If all children of $v$ have already been explored, then we pick a child, $v'$, which maximizes

\begin{equation}
\label{eq:uctnodevalue}
S(v') = \frac{Q(v')}{N(v')} + c\sqrt{\frac{2\ln{N(v)}}{N(v')}}
\end{equation} 
where $v'$ is a child of $v$, $N$ is the visit count and $Q$ is the accumulated score for a node (we will see later how to keep track of $Q$ and $N$, for a given node).
The parameter $c$ determines the amount of exploration. We will choose $c = 1 / \sqrt 2$ as per the comments in \citep[p. 9]{mcts_survey12}.

This selection process continues until we find either an unexplored node or run into a node without children (i.e. a leaf node), in which case we return that leaf node.

\subsubsection{The exploration step}

When we have found a node using the selection step, we will explore that node, which will yield a score.
If we are ``exploring'' a leaf node, the score will just be the value at the leaf node.
Otherwise, we are exploring an unexplored non-leaf node, and then simply search randomly from that node until we run into a leaf node, which we know how to evaluate a score for.
If the node has not previously been explored, it will finally be marked as explored, and it's visit count and score will be initialized as appropriate.

\subsubsection{The backup step}

When the exploration is done  we go back up the way we came, all the way to the root node.
As we go, we update visit count, $N$, and accumulate the score, $Q$.
We also make sure to alternate the sign of the score we use to accumulate as we go up the tree, since it represents the value for the player who's move it is at that node.

\subsection{An example}
In this section, we give an example of the UCT algorithm described above.
To keep things simple, we do only a single iteration.
However, in order to get a non-trivial iteration, we assume that five iterations have already been done, and do iteration number six.
\begin{center}
\def\arraystretch{5.5}
\begin{table}
\begin{tabular}{l}
  \def\svgwidth{\columnwidth} \input{mcts_example_before.pdf_tex}
\end{tabular}
\caption{Before the iteration}
\label{tab:mcts_iteration_before}
\end{table}
\end{center}
In table \ref{tab:mcts_iteration_before}, we have our starting point.
We must first carry out the selection step. As can be seen from the figure, there are two cases: one with $N=1$ and $Q=-1$, and one with $N=1$ and $Q=1$.
Let the choices on the second row of table \ref{tab:mcts_iteration_before} be $v_1$, $v_2$, $v_3$ and $v_4$, from left to right.
Using expression \ref{eq:uctnodevalue} with $c = 1/\sqrt{2}$, we get $S(v_1) = \sqrt{\ln{5}} - 1$ for the first choice, and $S(v_2) = S(v_3) = S(v_4) = \sqrt{\ln{5}} + 1$ for the other three choices.
So $v_2$, $v_3$ and $v_4$ all maximize $S$.
Since this node has no explored children, we do not need to calculate \ref{eq:uctnodevalue} -- we simply select the first child, i.e. the fourth node from the left, on the third row.
This node is the result of the selection step.

Now it is time to perform the exploration step.
This is just a random search from the selected node, until we hit a leaf node.
As can be seen from table \ref{tab:mcts_iteration_before}, there are two possible outcomes: either the seventh node on the fourth row, or the eight node on the fifth row.
Suppose that the outcome is the latter.
This node is a win for First.
We should now perform the backup step. We start with the score $1$, and alternate signs until we come back up to the selected node. Which gets initialized with $N=1$ and $Q=(-1)^2 = 1$.
The backup step is not done yet: we should continue all the way up to the root node, remembering to alternate the sign and adjust the accumulated scores and visit counts as we go.
The complete result after the backup step is shown in table \ref{fig:mcts_iteration_after}.
\begin{center}
\def\arraystretch{5.5}
\begin{table}
\begin{tabular}{l}
  \def\svgwidth{\columnwidth} \input{mcts_example_after.pdf_tex}
\end{tabular}
\caption{After the iteration}
\label{fig:mcts_iteration_after}
\end{table}
\end{center}

\subsection{UCT implementation}

It is assumed that we have a function, \texttt{value}, which can only be applied to terminal values and which gives a real number representing the value of a given node from the point of view of the player in turn (i.e. the opponent of the player who made the previous move). The value is $1$ if the position is a winning position for the player in turn, $-1$ if it is a losing position, and $0$ otherwise.
We also assume that we have a function named \texttt{choices}, which we can apply to a node in order to find the set of possible choices of nodes that the player in turn could move to. Finally, we assume that our nodes may carry ``MCTS data'' of the following format.
\begin{lstlisting}[frame=single]
data MCTSNodeData = MCTSNodeData { visitCount :: Int,
                                   score :: Score }
\end{lstlisting}
A node only carries \texttt{MCTSNodeData} if it has been explored by the \texttt{explore} function, below.
The \texttt{Score} type is just a synonym for a real number type, like \texttt{Float}. The function \texttt{getMCTSData} will return the MCTS data for a given node if it has any, and we can set it by means of \texttt{setMCTSData}.

We begin by looking at the \texttt{recon} function, which denotes the score \emph{from the point of view of the player in turn}, of a random search from the given position.
\begin{minipage}{\linewidth}
\begin{lstlisting}[frame=single]
recon position
  | Game.terminal position = return $ value pos
  | otherwise = do
      c <- Random.fromList [(c,1)
                           | c <- Game.choices position]
      s <- recon c
      return $ -s
\end{lstlisting}
\end{minipage}
It reads as follows: in case the given position is terminal, then we can return the \texttt{value} of that position. Otherwise we select a random child, apply \texttt{recon} to it, and get a score, \texttt{s}, back.
Note that $\texttt{s}$ represents the score from the point of view of the opponent of the player in turn, since \texttt{recon} was applied to a child of \texttt{position}. Therefore we must negate the score before we return it.

Next, we'll look at the \texttt{explore} function, which is the core of the algorithm. The return value of the function is a tuple of a score together with the explored node. Just like \texttt{recon}, the score it returns is relative to the player who's turn it is in the given node. Apart from the node to explore, it also takes \texttt{cExp}, which is just the $c$ parameter from expression~\ref{eq:uctnodevalue}.

\begin{minipage}{\linewidth}
\begin{lstlisting}[frame=single]
explore cExp node = 
  case getMCTSData node of
    Nothing -> do
      s <- if Game.terminal node
           then return $ value node
           else recon node
      return ( s,
               setMCTSData node $
               MCTSNodeData {visitCount = 1,
                             score = s} )
    Just (MCTSNodeData {visitCount = vc,
                        score = sc}) -> do
      case Game.terminal node of
        True -> do
          let s = value node
          return ( s,
                   setMCTSData node $
                   MCTSNodeData {visitCount = vc + 1,
                                 score = sc + s} )
        False -> do
          let (c, cs) = popBestChild cExp node
          (s, c') <- explore cExp c
          let s' = negate s
              node' = setChoices node (c':cs) in
            return ( s',
                     setMCTSData node' $
                     MCTSNodeData {visitCount = vc + 1,
                                   score = sc + s'} )
\end{lstlisting}
\end{minipage}
At the top level, the function is split up into two cases -- either our node has no MCTS data, or it does.
If the node does not have MCTS data, we obtain a score, \texttt{s}, in either of two ways: using the \texttt{value} function if the node is terminal, or else by applying the \texttt{recon} to the node, i.e. doing a random search.
In either case we get a score, and so we can return the explored version of our node.

In case the node has MCTS data, i.e. is explored, we again have two sub-cases: terminal or not terminal.
If the explored node is terminal, we again use the value function to obtain a score, and use that to update the MCTS data for the node.
If the explored node is not terminal, then we select the best child as described in section~\ref{subsec:uct_selection_step}, explore that child recursively with another call to \texttt{explore} and use the result to return an updated node.

One important ingredient is the \texttt{compareChildren} function. It takes a node and two children, \texttt{a} and \texttt{b}, of the node, and returns an ordering, which is just a type for representing ``less than'', ``equal to'' or ``greater than''.
It allows us to sort a set of children, and therefore to write \texttt{popBestChild} and \texttt{findBestMove} (below), with relative ease.
The function is just a straight encoding of the rules mentioned in \ref{subsec:uct_selection_step}.
\begin{lstlisting}[frame=single]
compareChildren cExp node a b =
  case (getMCTSData a, getMCTSData b) of
    (Just aData, Just bData) ->
      let Just parentData = getMCTSData node in
      compare (reconScore parentData aData) (reconScore parentData bData)    
      where
        reconScore :: MCTSNodeData -> MCTSNodeData -> Score
        reconScore parentData childData =
          let (vcp, sp) = (visitCount $ parentData, score $ parentData)
              (vcc, sc) = (visitCount $ childData, score $ childData) in
          ( sc / (fromIntegral vcc) ) +
          cExp * sqrt (  2.0*(log $ fromIntegral vcp) / (fromIntegral vcc)  )
    (Nothing, Nothing) -> EQ
    (Just _, Nothing) -> LT
    (Nothing, Just _) -> GT
\end{lstlisting}
The function splits into two cases. In the case where both children have been explored (i.e. have got MCTS data), we use expression~\ref{eq:uctnodevalue} to produce two numbers which us an ordering in the usual way. In case neither child have been explored, they are equal. In case the first has been explored but not the second, then the first is less than the second, and the final case is just the reverse of this.

With the \texttt{explore} function and \texttt{compareChildren} in hand, we can easily write the final MCTS strategy. It needs two parameters: a number of iterations and a node to work on.

\begin{minipage}{\linewidth}
\begin{lstlisting}[frame=single]
mctsStrategy 0 node = do
  return $ findBestMove node
mctsStrategy numSteps node = do
  (_, node') <- explore cExp node
  mctsStrategy (numSteps-1) node'
  where
  cExp :: Double  
  cExp = 1.0 / (sqrt 2.0)
\end{lstlisting}
\end{minipage}
The base case of zero iterations is listed first. It uses the function \texttt{findBestMove} which determines the best move for the player in turn, according to the ordering implied by \texttt{compareChildern}. In the other case, one round of \texttt{explore} is executed, and the strategy is applied recursively to the resulting node.

There are still a few minor blanks left to fill in. For the complete code, see section~\ref{sec:code}.
